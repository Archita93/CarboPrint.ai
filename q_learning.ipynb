{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50764aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION Reduce by 10%, 20% etc -> increase action space\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "possible_actions = [\"increase by level 1\",\"increase by level 2\",\"increase by level 3\",\"maintain\",\n",
    "           \"decrease by level 1\",\"decrease by level 2\",\"decrease by level 3\"]\n",
    "\n",
    "action_for_each_state = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "454cefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOUBTS: Should co2 be considered a state space\n",
    "# state_space1 = (gamma, pue, tdp_watts, config, chips) \n",
    "# state_space2 = (gamma1, pue1, tdp_watts1, config1, chips1) \n",
    "# difference = ce.state1 - ce.state2 \n",
    "# difference is positive -> + reward\n",
    "# difference is negative -> - reward  \n",
    "\n",
    "\n",
    "\n",
    "# goal -> 395-405 -> terminal state reached \n",
    "# budget limit = 100\n",
    "# current e = 500\n",
    "\n",
    "# diff = 400\n",
    "\n",
    "\n",
    "\n",
    "# Environment Implementation \n",
    "class env():\n",
    "    # start from state and then take an action to return next state and the reward in the next state\n",
    "    def __init__(self, curr_state,termination_co2):\n",
    "        # 7 actions can be taken \n",
    "        self.action_space = Discrete(7)  \n",
    "        # set start state\n",
    "        \n",
    "        self.curr_state = curr_state\n",
    "        self.termination_co2 = termination_co2\n",
    "        self.info_action = (0,0,0,0,0)\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "    def carbon_emissions(self,curr_state):\n",
    "        product = 1\n",
    "        # state_space1 = (gamma, pue, tdp_watts, config, chips) \n",
    "        for i in range(len(curr_state)):\n",
    "            product *= curr_state[i]\n",
    "        return product\n",
    "    \n",
    "       \n",
    "    def step(self,curr_state):\n",
    "        # state_space1 = (gamma, pue, tdp_watts, config, chips) \n",
    "        # info_action = (0,4,5,6,3)\n",
    "        reward = 0\n",
    "        prev_co2 = self.carbon_emissions(curr_state)\n",
    "        \n",
    "        # each state component takes an action \n",
    "        state_list = list(curr_state)\n",
    "        actions_list = list(self.info_action)\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_list[i] += actions_list[i] \n",
    "            \n",
    "        self.curr_state = tuple(state_list)\n",
    "        \n",
    "        new_state = curr_state\n",
    "        \n",
    "        new_co2 = self.carbon_emissions(new_state)\n",
    "        \n",
    "        old_diff = abs(self.termination_co2 - prev_co2)\n",
    "        new_diff = abs(self.termination_co2 - new_co2)\n",
    "        \n",
    "        if new_diff <= 0.1*self.termination_co2:\n",
    "            done = True\n",
    "        else:\n",
    "            # 100 - 500 = 400 -> old diff\n",
    "            # 100 - 200 = 200 -> new diff \n",
    "            # 200 - 400  = - 200 \n",
    "            if new_diff - old_diff >0: \n",
    "                reward -=1\n",
    "            else:\n",
    "                reward += 1\n",
    "            done = False\n",
    "        # info could be actions_list ?????????\n",
    "        info = {}\n",
    "        return self.curr_state, reward, done, info\n",
    "        \n",
    "    # difference between reset and init\n",
    "    def reset(self, curr_state,termination_co2):\n",
    "        self.action_space = Discrete(7)   \n",
    "        \n",
    "        self.curr_state = curr_state\n",
    "        \n",
    "        self.termination_co2 = termination_co2\n",
    "        \n",
    "        self.info_action = (0,0,0,0,0)\n",
    "        \n",
    "        return curr_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8fb180c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 2, 3, 4, 5), 1, False, {})\n"
     ]
    }
   ],
   "source": [
    "state = (1,2,3,4,5)\n",
    "env1 = env(state,100)\n",
    "prod = env1.step(state)\n",
    "print(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e76c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(np.array(5).prod(), 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 3),\n",
    "        )\n",
    "    # x is state\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34f9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0586a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90118647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf5492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb745e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a02c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the state space\n",
    "states = ['gamma', 'pue', 'grid_config','tdp_watts','chips_num',\"co2_emissions\"]\n",
    "state_space = [0,1,2,3,4]  # representing low, medium, high expenses for each category\n",
    "\n",
    "# Define the action space - will it mention the extent of change???? gamma, pue, tdp_watts needs to be more specific \n",
    "actions = ['reduce by 1 level', 'maintain', 'increase']    # elaborate\n",
    "\n",
    "#numeric action codes: 0 = reduce, 1 = maintain, 2 = increase\n",
    "action_space = [0,1,2]  # representing reducing, maintaining, or increasing expenses for each category\n",
    "\n",
    "\n",
    "\n",
    "# # 40 * 8 * 221 = 320*221 = 70,720 * 20 * 100 = 141,440,000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Doubt:    Reward for the goal????\n",
    "\n",
    "# initial state: The current co2 emissions of the model\n",
    "# terminal state: co2 emission reduced to the given budget (+-5%)\n",
    "\n",
    "goal_state = 100 \n",
    "initial_state = 500 \n",
    "\n",
    "\n",
    "# self.network = nn.Sequential(\n",
    "#     nn.Linear(np.array(env.single_observation_space.shape).prod(), 120),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(120, 84),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(84, env.single_action_space.n),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ba0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/dqn/#dqnpy\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(np.array(5).prod(), 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 3),\n",
    "        )\n",
    "    # x is state\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python dqn.py --total-timesteps 500000 \\ 500 \n",
    "#     --learning-rate 2.5e-4 \\\n",
    "#     --buffer-size 10000 \\\n",
    "#     --gamma 0.99 \\\n",
    "#     --target-network-frequency 500 \\ 0.5\n",
    "#     --max-grad-norm 0.5 \\\n",
    "#     --batch-size 128 \\\n",
    "#     --start-e 1 \\\n",
    "#     --end-e 0.05 \\\n",
    "#     --exploration-fraction 0.5 \\\n",
    "#     --learning-starts 10000 ???// 10\n",
    "#     --train-frequency 10 ????? 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a67d14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class env():\n",
    "    # start from state and then take an action to return next state and the reward in the next state\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def reset(self):\n",
    "        return \n",
    "    def step(self, action):\n",
    "        return reward_in_next_state, next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fab3922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "    q_network = QNetwork().to(device)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=2.5e-4)\n",
    "    target_network = QNetwork().to(device)\n",
    "    \n",
    "    # resume the training -> checkpoint -> starts from checkpoint\n",
    "    target_network.load_state_dict(q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473b655",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Seed -> random action probability to encourage exploration -> reproducablility -> controls the randomness\n",
    "obs, _ = envs.reset(seed=args.seed)\n",
    "    for global_step in range(500):\n",
    "        # ALGO LOGIC: put action logic here\n",
    "        epsilon = linear_schedule(1, 0.05, 0.5 * 500, global_step)\n",
    "        if random.random() < epsilon:\n",
    "            # actions space in an array (np.array)\n",
    "            actions = np.array([actions for _ in range(3)])\n",
    "        else:\n",
    "            q_values = q_network(torch.Tensor(obs).to(device))\n",
    "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        # next_observation -> how to initialise observations\n",
    "        # rewards: where do we initalise rewards\n",
    "        # truncated, infos came from \n",
    "        # terminated: +-5% of the desired budget, so how do we formulate that in the code\n",
    "        next_obs, rewards, terminated, truncated, infos = envs.step(actions)\n",
    "\n",
    "#         # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "#         if \"final_info\" in infos:\n",
    "#             for info in infos[\"final_info\"]:\n",
    "#                 # Skip the envs that are not done\n",
    "#                 if \"episode\" not in info:\n",
    "#                     continue\n",
    "#                 print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "#                 writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "#                 writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "#                 writer.add_scalar(\"charts/epsilon\", epsilon, global_step)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Doubts ????? DO WE REALLY NEED BUFFER?\n",
    "#         # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "#         real_next_obs = next_obs.copy()\n",
    "#         for idx, d in enumerate(truncated):\n",
    "#             if d:\n",
    "#                 real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "#         rb.add(obs, real_next_obs, actions, rewards, terminated, infos)\n",
    "\n",
    "#         # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "#         obs = next_obs\n",
    "\n",
    "        # ALGO LOGIC: training.\n",
    "        if global_step > 10:\n",
    "            if global_step % 2 == 0:\n",
    "                #Doubt: Buffer rb. sample, batch size??????\n",
    "                data = rb.sample(args.batch_size)\n",
    "                \n",
    "                \n",
    "                # DOUBTS: What is this code doing??????\n",
    "                with torch.no_grad():\n",
    "                    target_max, _ = target_network(data.next_observations).max(dim=1)\n",
    "                    td_target = data.rewards.flatten() + 0.99 * target_max * (1 - data.dones.flatten())\n",
    "                old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
    "                loss = F.mse_loss(td_target, old_val)\n",
    "\n",
    "#                 if global_step % 100 == 0:\n",
    "#                     writer.add_scalar(\"losses/td_loss\", loss, global_step)\n",
    "#                     writer.add_scalar(\"losses/q_values\", old_val.mean().item(), global_step)\n",
    "#                     print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "#                     writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "                # optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # update target network\n",
    "            \n",
    "            # DOUBT: What is tau - kept it as 1 for now\n",
    "            if global_step % 0.5 == 0:\n",
    "                for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
    "                    target_network_param.data.copy_(\n",
    "                        args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data\n",
    "                    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
